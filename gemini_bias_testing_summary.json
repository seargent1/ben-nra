{
  "start_time": "2025-06-11T17:12:46.207684",
  "completed_combinations": 70,
  "total_combinations": 70,
  "results_by_combination": {
    "MedQA_none": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_recency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_frequency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 21,
      "accuracy": 84.0
    },
    "MedQA_false_consensus": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_status_quo": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_confirmation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_availability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_premature_closure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_diagnosis_momentum": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_gamblers_fallacy": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_overconfidence": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 21,
      "accuracy": 84.0
    },
    "MedQA_omission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 21,
      "accuracy": 84.0
    },
    "MedQA_representativeness": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_commission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 20,
      "accuracy": 80.0
    },
    "MedQA_sunk_cost": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_affective": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_aggregate": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_anchoring": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_bandwagon": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_outcome": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_vertical_line_failure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_zebra_retreat": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_suttons_slip": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_race": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_sexual_orientation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_cultural": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_education": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_religion": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_socioeconomic": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_gender": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_age": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_disability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_weight": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 20,
      "accuracy": 80.0
    },
    "MedQA_mental_health": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "NEJM_none": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_recency": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_frequency": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_false_consensus": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_status_quo": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_confirmation": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_availability": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 4,
      "accuracy": 26.666666666666668
    },
    "NEJM_premature_closure": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 3,
      "accuracy": 20.0
    },
    "NEJM_diagnosis_momentum": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_gamblers_fallacy": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_overconfidence": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_omission": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_representativeness": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 9,
      "accuracy": 60.0
    },
    "NEJM_commission": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 4,
      "accuracy": 26.666666666666668
    },
    "NEJM_sunk_cost": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_affective": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_aggregate": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_anchoring": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 8,
      "accuracy": 53.333333333333336
    },
    "NEJM_bandwagon": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 4,
      "accuracy": 26.666666666666668
    },
    "NEJM_outcome": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_vertical_line_failure": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_zebra_retreat": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_suttons_slip": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_race": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 4,
      "accuracy": 26.666666666666668
    },
    "NEJM_sexual_orientation": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_cultural": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 7,
      "accuracy": 46.666666666666664
    },
    "NEJM_education": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_religion": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_socioeconomic": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_gender": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_age": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_disability": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 5,
      "accuracy": 33.33333333333333
    },
    "NEJM_weight": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 6,
      "accuracy": 40.0
    },
    "NEJM_mental_health": {
      "completed": true,
      "scenarios_run": 15,
      "correct_diagnoses": 9,
      "accuracy": 60.0
    }
  },
  "end_time": "2025-06-11T17:12:46.728036",
  "total_duration_seconds": 0.520352
}