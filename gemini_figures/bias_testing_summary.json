{
<<<<<<< Updated upstream
  "start_time": "2025-05-24T21:35:13.493962",
  "completed_combinations": 70,
  "total_combinations": 70,
  "results_by_combination": {
    "MedQA_none": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 20,
      "accuracy": 80.0
    },
    "MedQA_recency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_frequency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_false_consensus": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_status_quo": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_confirmation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_availability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_premature_closure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 12,
      "accuracy": 48.0
    },
    "MedQA_diagnosis_momentum": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_gamblers_fallacy": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_overconfidence": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 14,
      "accuracy": 56.00000000000001
    },
    "MedQA_omission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_representativeness": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_commission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 18,
      "accuracy": 72.0
    },
    "MedQA_sunk_cost": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_affective": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_aggregate": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_anchoring": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_bandwagon": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_outcome": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_vertical_line_failure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_zebra_retreat": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_suttons_slip": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_race": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 14,
      "accuracy": 56.00000000000001
    },
    "MedQA_sexual_orientation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_cultural": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_education": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 17,
      "accuracy": 68.0
    },
    "MedQA_religion": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_socioeconomic": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_gender": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_age": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 15,
      "accuracy": 60.0
    },
    "MedQA_disability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 16,
      "accuracy": 64.0
    },
    "MedQA_weight": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "MedQA_mental_health": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 19,
      "accuracy": 76.0
    },
    "NEJM_none": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 13,
      "accuracy": 52.0
    },
    "NEJM_recency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_frequency": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_false_consensus": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 8,
      "accuracy": 32.0
    },
    "NEJM_status_quo": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_confirmation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 12,
      "accuracy": 48.0
    },
    "NEJM_availability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    },
    "NEJM_premature_closure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_diagnosis_momentum": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 12,
      "accuracy": 48.0
    },
    "NEJM_gamblers_fallacy": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_overconfidence": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_omission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    },
    "NEJM_representativeness": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_commission": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_sunk_cost": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_affective": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_aggregate": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 12,
      "accuracy": 48.0
    },
    "NEJM_anchoring": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_bandwagon": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 6,
      "accuracy": 24.0
    },
    "NEJM_outcome": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 8,
      "accuracy": 32.0
    },
    "NEJM_vertical_line_failure": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_zebra_retreat": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_suttons_slip": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 8,
      "accuracy": 32.0
    },
    "NEJM_race": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    },
    "NEJM_sexual_orientation": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_cultural": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 11,
      "accuracy": 44.0
    },
    "NEJM_education": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 8,
      "accuracy": 32.0
    },
    "NEJM_religion": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    },
    "NEJM_socioeconomic": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_gender": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 9,
      "accuracy": 36.0
    },
    "NEJM_age": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    },
    "NEJM_disability": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 12,
      "accuracy": 48.0
    },
    "NEJM_weight": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 7,
      "accuracy": 28.000000000000004
    },
    "NEJM_mental_health": {
      "completed": true,
      "scenarios_run": 25,
      "correct_diagnoses": 10,
      "accuracy": 40.0
    }
  },
  "end_time": "2025-05-24T21:35:13.908599",
  "total_duration_seconds": 0.414637
=======
  "start_time": "2025-06-05T19:42:49.443946",
  "completed_combinations": 0,
  "total_combinations": 70,
  "results_by_combination": {},
  "end_time": "2025-06-05T19:42:49.531269",
  "total_duration_seconds": 0.087323
>>>>>>> Stashed changes
}